


# run this
get_ipython().getoutput("pip list")








import pandas as pd


data1 = pd.read_csv("dataset1.csv")
data1.head()





get_ipython().run_line_magic("matplotlib", " inline")
import matplotlib.pyplot as plt


f, ax = plt.subplots(figsize=(10, 9))

data1["Country_index"] = 
ax.scatter(data1.Longitude, data1.Latitude, marker=".", c=data1.Monastery_index, cmap="rainbow")





import numpy as np


f2, ax2 = plt.subplots(figsize=(9, 6))
bins = np.arange(1000, 1700, 25)

ax2.hist(data1.Starting, bins=bins)





f3, ax3 = plt.subplots(1, 2, figsize=(14, 6))
bins0 = np.arange(1000, 1500, 20)
ax3[0].hist(data1.loc[data1["Monastery"]=="Cistercians", "Starting"], bins=bins0, label="Cistercian", color="orange")
bins1 = np.arange(1200, 1600, 20)
ax3[1].hist(data1.loc[data1["Monastery"]=="Franciscans", "Starting"], bins=bins1, label="Franciscan")
f3.legend(title="Legend:", title_fontsize=14, fontsize=14)
f3.suptitle("Starting Year", fontsize=20)





f4, ax4 = plt.subplots(figsize=(9, 6))
bins = np.arange(1000, 2100, 100)

ax4.hist(data1.Ending, bins=bins)


f5, ax5 = plt.subplots(1, 2, figsize=(14, 6))
bins0 = np.arange(1000, 2100, 100)
ax5[0].hist(data1.loc[data1["Monastery"]=="Cistercians", "Ending"], bins=bins0, label="Cistercian", color="orange")
bins1 = np.arange(1300, 2100, 80)
ax5[1].hist(data1.loc[data1["Monastery"]=="Franciscans", "Ending"], bins=bins1, label="Franciscan")
f5.legend(title="Legend:", title_fontsize=14, fontsize=14)
f5.suptitle("Ending Year", fontsize=20)





counts = {}
for i, row in data1.iterrows():
    con = row["Country"]
    try:
        counts[con] += 1
    except:
        counts[con] = 1
counts = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))

f6, ax6 = plt.subplots(figsize=(20, 6))
ax6.bar(counts.keys(), counts.values())








x = data1[["Starting", "Ending"]]
y = data1["Monastery_index"]





from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, stratify=y)





from sklearn.linear_model import Perceptron
P = Perceptron(max_iter=50, eta0=0.1)
P.fit(x_train, y_train)
y_pred = P.predict(x_test)


wrong = 0
y_test = y_test.reset_index(drop=True)

for i in range(len(y_pred)):
    if y_pred[i] != y_test[i]:
        wrong += 1
mislabeled = wrong/len(y_pred)

print(f"{mislabeled:.4f} mislabeled (4dp)")


from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap="Blues")


from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score

print(f"precision score of the model is {precision_score(y_test, y_pred):4f}")
print(f"recall score of the model is {recall_score(y_test, y_pred):4f}")
print(f"accuracy score of the model is {accuracy_score(y_test, y_pred):4f}")
print(f"f1 score of the model is {f1_score(y_test, y_pred):4f}")





from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(fit_intercept=True)
LR.fit(x_train, y_train)
y_pred = LR.predict(x_test)


wrong = 0
y_test = y_test.reset_index(drop=True)

for i in range(len(y_pred)):
    if y_pred[i] != y_test[i]:
        wrong += 1
mislabeled = wrong/len(y_pred)

print(f"{mislabeled:.4f} mislabeled")


ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap="Reds")


print(f"precision score of the model is {precision_score(y_test, y_pred):.4f}")
print(f"recall score of the model is {recall_score(y_test, y_pred):.4f}")
print(f"accuracy score of the model is {accuracy_score(y_test, y_pred):.4f}")
print(f"f1 score of the model is {f1_score(y_test, y_pred):.4f}")








# Train-test-split is important in order to have training data which the model can "learn" from and separate testing data which the
# model can then be judged on since it has never seen this data before. This is necessary to improve models with repeated stages
# of training and testing and/or to compare the performance of different models/ same model with different parameters etc.





# Logistic Regression performed better, from general overview of confusion matrix you can see Logistic Regression model provided
# much higher proportion of correct predictions compared with the Perceptron Model more specifically, it had much higher
# precision and accuracy scores which means Logistic Regression had more correct positive predictions of all those classified
# as positive as well as more correct predictions (positive and negative) of all predictions made over the test data due to these
# major changes, f1 score of Logistic Regression is almost 10% higher than the Perceptron, generally indicating better performance





# they change slightly however comparatively this does not make a significant difference
# this is due to change in train-test-split data as while i have ensured the proportions of the two classes remain the same
# using stratify=y the actual data points still change and therefore the model has slightly different performance having
# and trained and then been tested differently than the first time, leading to different distribution of predictions on the
# classification matrix, which is where the scores are calculated from








data2 = pd.read_csv("dataset2.csv")
data2.head()


f, ax = plt.subplots(2, 2, figsize=(16, 11))

ax[0, 0].scatter(data2["fixed acidity"], data2["pH"], c=data2["quality"])
ax[0, 0].update({'xlabel': 'fixed acidity', 'ylabel': 'pH'})

ax[0, 1].scatter(data2["total sulfur dioxide"], data2["free sulfur dioxide"], c=data2["quality"])
ax[0, 1].update({'xlabel': 'total sulfur dioxide', 'ylabel': 'free sulfur dioxide'})

ax[1, 0].scatter(data2["chlorides"], data2["alcohol"], c=data2["quality"])
ax[1, 0].update({'xlabel': 'chlorides', 'ylabel': 'alcohol'})

ax[1, 1].scatter(data2["fixed acidity"], data2["density"], c=data2["quality"])
ax[1, 1].update({'xlabel': 'fixed acidity', 'ylabel': 'density'})





from sklearn.feature_selection import r_regression
print(f"Pearson's correlation coefficient between fixed acidity and pH is {r_regression(data2, data2['pH'])[0]:.4f}")
print(f"Pearson's correlation coefficient between total sulfur dioxide and free sulphur dioxide is {r_regression(data2, data2['free sulfur dioxide'])[6]:.4f}")
print(f"Pearson's correlation coefficient between chlorides and alcohol is {r_regression(data2, data2['alcohol'])[4]:.4f}")
print(f"Pearson's correlation coefficient between fixed acidity and density is {r_regression(data2, data2['density'])[0]:.4f}")





X = data2[["fixed acidity", "alcohol", "density"]]
y = data2["quality"]


from sklearn.linear_model import LinearRegression

L = LinearRegression(fit_intercept=True, copy_X=True)
L.fit(X, y)
L.score(X, y)


L.intercept_, L.coef_





from sklearn.model_selection import KFold

K = 10
kf = KFold(n_splits=K, shuffle=True)
r2s = []
X_split = kf.split(X)


for fold, (train_i, test_i) in enumerate(X_split):
    x_train = X.iloc[train_i]
    y_train = y.iloc[train_i]
    L.fit(x_train, y_train)
    r2s.append(L.score(X, y))

r2s = pd.DataFrame(r2s)
print(f"mean is {r2s.mean()[0]:.4f} to 4dp")
print(f"std is {r2s.std()[0]:.4f} to 4dp")





# 18)
# using more columns would give more accurate score between quality and the rest of the features as a whole but would not
# necessarily be higher since any features with extreme (poor or excellent) R2 score with quality would skew the final
# calculated mean and std of the R2 score, more specifically a feature with poor R2 score with quality would reduce the mean
# R2 score so using more/ all the features doesn't necessarily increase R2 score





# 19)
# since the R2 scores are not that different and generally they of acceptable level, then other factors such as computational
# weight of the model should be taken into account
# for example R2=0.8 is more prefereable if the model required less features/ had lower space/ time complexity than R2=0.9 model
# since acceptable results can still be achieved with this model without wasting computational resources
# similar argument can be made for classifiers where lower accuracy can still be favourable if acceptable results are still
# produced on test data while saving time/ memory/ computational power etc.








data = pd.read_csv("dataset3.csv")
data.head()


X = data[["Longitude", "Latitude"]]


from sklearn.cluster import KMeans
kms = []
for k in [5, 10, 15]:
    kmeans = KMeans(n_clusters=k)
    kms.append(kmeans)


f, ax = plt.subplots(3, 1, figsize=(13, 30))
for i in range(len(kms)):
    ax[i].scatter(X["Longitude"], X["Latitude"], marker=".", c=kms[i].fit_predict(X), cmap="tab20")





from sklearn.cluster import DBSCAN
dbs = []
for e in [0.001, 0.005, 0.01, 0.05]:
    db = DBSCAN(eps=e)
    dbs.append(db)


f, ax = plt.subplots(4, 1, figsize=(13, 40))
for i in range(len(dbs)):
    ax[i].scatter(X["Longitude"], X["Latitude"], c=dbs[i].fit_predict(X), cmap="tab20")





from sklearn.metrics import silhouette_score

print("kmeans:")
k = [5, 10, 15]
for i in range(len(k)):
    labels = kms[i].fit_predict(X)
    print(k[i], end="-> ")
    print(silhouette_score(X, labels, metric="euclidean"))

print("dbscans:")
e = [0.001, 0.005, 0.01, 0.05]
for i in range(len(e)):
    labels = dbs[i].fit_predict(X)
    print(e[i], end="-> ")
    print(silhouette_score(X, labels, metric="euclidean"))



# best clustering using silhouette score is highest values (closest to +1) therefore best model for clustering this data
# was k=15 KMeans clustering model with this test





from sklearn.metrics import davies_bouldin_score

print("kmeans:")
k = [5, 10, 15]
for i in range(len(k)):
    labels = kms[i].fit_predict(X)
    print(k[i], end="-> ")
    print(davies_bouldin_score(X, labels))

print("dbscans:")
e = [0.001, 0.005, 0.01, 0.05]
for i in range(len(e)):
    labels = dbs[i].fit_predict(X)
    print(e[i], end="-> ")
    print(davies_bouldin_score(X, labels))



# # best clustering using davies-bouldin score is lowest values (closest to 0) therefore best model for clustering this data
# was also shown to be k=15 KMeans clustering model with this test





from sklearn.cluster import AgglomerativeClustering


for n in range(3, 13, 3):
    agg_clust = AgglomerativeClustering(n_clusters=n, affinity="euclidean", linkage="ward")
    agg_clust.fit_predict(X)
    print(n, "->", silhouette_score(X, agg_clust.labels_, metric="euclidean"))
    print(n, "->", davies_bouldin_score(X, agg_clust.labels_))


# with silhouette score, n=12 Agglomerative Clustering produces better results since 0.64 > 0.62
# with davies bouldin score, k=15 KMeans clustering remains as the most better clustering model for the data





# the smaller the eps parameter, the smaller the radius the model evaluates valid addition to the current cluster for the
# longer it takes becasue there are more calculations necessary and the algorithm requires more time to cover all the data points
# in the sample given since its looking for neighbours within given radius from every datapoint.
# whereas kmeans is given a fixed number of clusters (k) to optimize by randomly placing centroids and minimising distances to
# the surrounding datapoints meaning it is more efficient for larger datasets as this is much less calulations necessary.





# for data where clusters are of consistant density but irregular shapes, DBSCAN will perform better since it will traverse
# the data points for neighbours in a certain range, relying on density to determine valid clustering.
# on the other hand, clusters which are of regular (circular) shape but not necessarily evenly distributed/ density, KMeans will
# perform better since it will rely on calculating distances from centroid points and not distances between neighbouring datapoints.
# and KMeans is also preferred if certain number of clusters is known/ needed in the dataset compared to DBSCAN where the
# number of clusters cannot be determined/ decided by the user before running the model on the data.








# Model A seems to be limited after 100 datapoints as it no longer improves, adding data is not likely to have much effect
# and both graphs are likely to continue to extend horizontally ie. the model will continue to make the same number of errors
# when tested
# Model B shows continuous improvement in testing with increased training set size so addition to the training set may show
# more errors as it learns however a more significant improvement is likely to be seen in testing resulting in less errors





# Model A seems to have high bias, the model is likely to be simple leading to high error due to underfitting and therefore
# poor training results which is reflected in error-prone test outcomes, can be improved by increasing the model's complexity
# to allow for more degrees of freedom.
# Model B is likely to well defined model with appropriate number of degrees of freedom for the data, leading to effective
# training which reflects in comparatively error-free testing results, would be improved by increasing training set size allowing
# model to increase accuracy.
# Model C seems to have high variance as the graph shows strong training results but weak testing results indicating a complex
# model which is overfitting and causing many errors in testing, could be improved by choosing a simpler model or reducing
# degrees of freedom in the current model.





# overfitting is when a model fits the training data too well leading to high error rate in testing, usually due to high
# variance, when a model tends to have high sensitivity to small fluctuations in the training set.
# this is problematic because the model becomes too specific and is no longer able to generalise the pattern/ trend in the data
# so performs badly in testing/ real life.
# to avoid this, while working on the model regularly test the model so that poor test results (perhaps categorised by preset
# threshold) is caught before the model is made even more complex leading to certain overfitting








# PCA is a very common method, it is computationally efficient and will produce a clear and meaningful dimensionally-reduced
# visualisation even for a complex high-D dataset however it assumes linearity and will therefore lose information when dealing
# non-linearly related features
# on the other hand, both t-SNE and UMAP will be able to handle these non-linear relationships, however UMAP will perform
# significantly better (with better space and time complexity) as well as producing a more robust visualisation which preserves
# local and global structures





# since preproessing involves cleaning and manipulating the reduced dataset, PCA is high recommended since it is reliable
# both for visualising dimensionality reduction and then being able to work with the data in that state, compared to either
# t-SNE of UMAP which are not generally recommended beyond visualisation
# however, limitations of PCA will have to be considered, for example PCA is well-suited for linear relationships whereas complex
# data is likely to have non-linear relationships, in addition, non-numeric data will have to be pre-processed and numerically
# represented in order for PCA to work correctly, this is time and resource heavy and will likely need to be done manually which
# may not be ideal for such a large scale dataset despite PCA itself being well-suited for large-scale applications





# while the exact representations of the dataset in lower dimentions is not very reproducible using t-SNE or UMAP, the
# general analysis is since a similar visualisation and conclusions can be achieved
# however the stability of these conclusions is questionable since they can change dramatically depending on the hyperparameters
# used and any misinterpretation made by considering cluster size/ distance or general shape produced in the visualisation
# as these are effectively meaningless features of the visualisation when using t-SNE or UMAP
# similarly, use of PCA,as stated previously, will limit its effectiveness with complex dataset since non-linear relationships
# are to be expected, however keeping this in mind, meaningful visualisation and analysis both can still be achieved








# in a senario where one class signficantly outweighs the other ie. the dataset is unbalanced
# in this case the accuracy score may seem very high since the model is performing well for majority of the dataset since it
# is mostly made up of the dominant class, this hides poor performance of the other class and can lead to bias when the
# model is deployed and tested, for example identifying fruadulent transactions however this are rare in comparison to genuine
# banking transactions etc.
# this is because accuracy score is general measure for the model as a whole, to find under-represented classes and misleading
# conclusions you should calculate precision and recall score per class as this will show the model's performance more clearly,
# in the example of fraudulent transactions, accuracy would only show the proportion of all predictions that are correct,
# largely skewed by the dominant non-fraudulent activity, whereas recall of fraudulent class will clearly show how many of the
# fraudulent transactions specifically are correctly identified etc.
# simply doing f1 score per class is also sufficient as this is single metric specific to each class and combines the precision
# and recall so is also more informative than only doing accuracy score in this type of scenario.





# precision is a measure of how many are correctly identified in all positive classifications, assuming there are mistakes
# ie. false positives
# recall is a measure of how many of the positive class are correctly identified as such, assuming there are mistakes ie.
# false negatives
# these two metrics are independent and would therefore require a perfect classifier in order to both be 100% because looking
# at the extreme, a classifier that consistently predicts a single class will have 100% recall but terrible precision and vice versa
# if positive classification is given sparingly (only when the model is super certain). in the general case, this means there
# has to be a trade-off between the two since achieving good precision leads to false negatives and achieving good recall leads to
# false positives, in other words unless a perfect classifier is build (highly unlikely) both precision and recall can't be 100%





# high recall and low precision is when all positive samples are well identified however there are many false positives,
# this may be desirable if patients are being tested for a disease and positive test result is needed for treatment, that way 
# almost every sick person is able to be treated but several healthy people may also be given the medicine etc.
# high precision and low recall is when positive classifications are correct however there are high numbers of false negatives,
# this is helpful if only the most certain samples should be identified for example if model is predicting best plant samples to 
# use in the next stage of artificial selection in order to genetically enhance a plant species for survival/ yield etc. in this
# case, the only the best of the best plants will be classified as usuable (positive) while many perfectly reasonable samples will be
# incorrectly identified as unusable (negative).





# a facial recognition model to detect expression of particular features etc. and trained on data collected in England, for example,
# may perform very well in Europe but will perform with horrible accuracy in China because of difference in features/ structure of
# caucasian vs oriental faces. While the model may be reliable within fixed training and testing within the context of predominantly
# white regions, it is not appropriate to use in real life or claiming that it works globally/ for everyone etc.
# this flaw should be recognised before release, either during preprocessing where imbalance of training dataset is recognised or
# during testing when realistic test data is used containing mix of individuals from all regions/ backgrounds (assuming the model
# was intended to be used on everyone by the creators) and poor performance can then be addressed.
# if the data used for training/ testing is limited and collection is difficult, the problem can be addressed by reducing the number
# samples of the dominant feature in the corpus (ie. removing some white people faces) or increasing the number of samples
# of the non-dominant feature (ie. augment the non-white individuals faces using mirroring to increase sample size) until there
# is roughly equal number of samples from each.





# for example a model being used to classify past criminals as high/ low-risk later in life which is trained on details about
# each individual, their previous activies and their background may pick up on high correlation between crime and regional or racial
# information about the individuals to wrongly assume their risk. a specific example may be people living in a certain area are black
# and their correlation with high crime may be used by the algorithm to always classify them as high-risk later in life
# while this trend may be consistent in the dataset used and therefore show low error when testing, this is a fundamentally wrong
# assumption since in real life it could be down to that area having generally low income leading to more crime.
# so black individuals in other regions are wrongly classified as high-risk as well as non-black individuals in that area
# who genuinely are a concern may not be given high-risk status, in other words this error is not caught in the testing of the model
# but would be problematic when applied in real life.
# this can be addressed by methods of feature extraction, both to remove unnecessary features but also to determine whether the
# model's accuracy is highly dependent on certain features which can then be examined for bias in real-world application
# similarly, expanding original dataset with more variation between such features (ie. reducing correlation) could allow the model
# to be less reliant on them and this can assessed with regular evaluation/ human oversight as should be done with any system
# dealing with sensitive/ human variables with real-world implications.



